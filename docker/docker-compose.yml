# Water Treatment Controller - Docker Compose
# Copyright (C) 2024
# SPDX-License-Identifier: GPL-3.0-or-later
#
# Port Configuration:
# All ports are configurable via environment variables.
# See config/ports.env for the single source of truth.
#
# Database Credentials (DEV/TEST ONLY):
# INTENTIONALLY HARDCODED - see CLAUDE.md
# - DB_USER: wtc
# - DB_PASSWORD: wtc_password (DO NOT change or externalize)
# - DB_NAME: water_treatment
# These appear in: database.environment.POSTGRES_PASSWORD, api.environment.DATABASE_URL
#
# Usage:
#   docker compose --env-file ../config/ports.env up -d
#
# Or set variables directly:
#   WTC_API_PORT=8000 WTC_UI_PORT=8080 docker compose up -d

services:
  # PostgreSQL with TimescaleDB
  database:
    image: timescale/timescaledb:latest-pg15
    container_name: wtc-database
    restart: unless-stopped
    environment:
      POSTGRES_USER: wtc
      POSTGRES_PASSWORD: wtc_password
      POSTGRES_DB: water_treatment
    volumes:
      - db_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    # DCK-C1 fix: Remove exposed ports - use internal network only
    expose:
      - "5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U wtc -d water_treatment"]
      interval: 10s
      timeout: 5s
      retries: 5
    # DCK-H3 fix: Add resource limits
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          memory: 256M

  # Web API
  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile.web
    container_name: wtc-api
    restart: unless-stopped
    # DCK-H1 fix: Run as non-root user
    user: "1000:1000"
    environment:
      DATABASE_URL: postgresql://wtc:wtc_password@database:${WTC_DB_PORT:-5432}/water_treatment
      API_HOST: 0.0.0.0
      WTC_API_PORT: ${WTC_API_PORT:-8000}
      WTC_UI_PORT: ${WTC_UI_PORT:-8080}
      # Development mode: lenient startup validation (downgrades path/UI failures to warnings)
      # Set to "production" for strict validation in production environments
      WTC_STARTUP_MODE: ${WTC_STARTUP_MODE:-development}
      # Skip IPC check when running without the C controller
      WTC_SIMULATION_MODE: ${WTC_SIMULATION_MODE:-true}
      # Skip UI asset validation (UI is served by separate container)
      WTC_API_ONLY: ${WTC_API_ONLY:-true}
    ports:
      - "${WTC_API_PORT:-8000}:${WTC_API_PORT:-8000}"
    depends_on:
      database:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${WTC_API_PORT:-8000}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    # DCK-H3 fix: Add resource limits
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          memory: 128M

  # PROFINET Controller (optional - requires host network mode)
  controller:
    build:
      context: ..
      dockerfile: docker/Dockerfile.controller
    container_name: wtc-controller
    restart: unless-stopped
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    environment:
      WTC_INTERFACE: ${WTC_INTERFACE:-eth0}
      WTC_CYCLE_TIME: ${WTC_CYCLE_TIME:-1000}
      WTC_LOG_LEVEL: ${WTC_LOG_LEVEL:-INFO}
    volumes:
      - ./config:/etc/water-controller:ro
      - controller_logs:/var/log/water-controller
    # DCK-H2 fix: Add health check (matches Dockerfile.controller)
    healthcheck:
      test: ["CMD", "pgrep", "water_treat_controller"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    # DCK-H3 fix: Add resource limits
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 512M
        reservations:
          memory: 128M
    # Only start if PROFINET interface is configured
    profiles:
      - profinet

  # React HMI Frontend
  ui:
    build:
      context: ..
      dockerfile: docker/Dockerfile.ui
      args:
        # Pass API_URL at build time for Next.js rewrites
        # Uses container-to-container name resolution
        API_URL: http://api:${WTC_API_PORT:-8000}
    container_name: wtc-ui
    restart: unless-stopped
    # DCK-H1 fix: Run as non-root user
    user: "1000:1000"
    environment:
      # API URL for server-side requests (container name resolution)
      API_URL: http://api:${WTC_API_PORT:-8000}
      # Public API URL for client-side (empty = use same origin with rewrites)
      NEXT_PUBLIC_API_URL: ""
      WTC_UI_PORT: ${WTC_UI_PORT:-8080}
      WTC_API_PORT: ${WTC_API_PORT:-8000}
    ports:
      # Map external UI port to internal container port (3000)
      - "${WTC_UI_PORT:-8080}:${WTC_DOCKER_UI_INTERNAL_PORT:-3000}"
    depends_on:
      api:
        condition: service_healthy
    # DCK-H2 fix: Add health check
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:${WTC_DOCKER_UI_INTERNAL_PORT:-3000}"]
      interval: 30s
      timeout: 10s
      retries: 3
    # DCK-H3 fix: Add resource limits
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          memory: 64M

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: wtc-grafana
    restart: unless-stopped
    # DCK-H1 fix: Run as grafana user
    user: "472:472"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      # Grafana Credentials (DEV/TEST ONLY):
      # INTENTIONALLY HARDCODED - see CLAUDE.md
      # - GF_SECURITY_ADMIN_USER: admin
      # - GF_SECURITY_ADMIN_PASSWORD: admin (DO NOT change or externalize)
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource
      GF_SERVER_HTTP_PORT: ${WTC_GRAFANA_PORT:-3000}
      # Unified alerting (disabled by default - enable when ready)
      GF_UNIFIED_ALERTING_ENABLED: ${GF_UNIFIED_ALERTING_ENABLED:-true}
      GF_ALERTING_ENABLED: "false"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    ports:
      - "${WTC_GRAFANA_PORT:-3000}:${WTC_GRAFANA_PORT:-3000}"
    depends_on:
      loki:
        condition: service_healthy
      database:
        condition: service_healthy
    # DCK-H2 fix: Add health check
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:${WTC_GRAFANA_PORT:-3000}/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    # DCK-H3 fix: Add resource limits
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          memory: 64M

  # Loki - Centralized log aggregation
  loki:
    image: grafana/loki:2.9.4
    container_name: wtc-loki
    restart: unless-stopped
    command: -config.file=/etc/loki/loki-config.yml
    volumes:
      - ./loki/loki-config.yml:/etc/loki/loki-config.yml:ro
      - loki_data:/loki
    expose:
      - "3100"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3100/ready"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          memory: 128M

  # Promtail - Log collector (ships Docker logs to Loki)
  promtail:
    image: grafana/promtail:2.9.4
    container_name: wtc-promtail
    restart: unless-stopped
    command: -config.file=/etc/promtail/promtail-config.yml
    volumes:
      - ./promtail/promtail-config.yml:/etc/promtail/promtail-config.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - promtail_positions:/var/promtail
    depends_on:
      loki:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9080/ready"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          memory: 32M

  # OpenPLC Editor (Read-Only Ladder Logic Viewer)
  # Provides browser-based ladder logic visualization with Modbus communication
  openplc:
    image: openplcproject/openplc_v3:latest
    container_name: wtc-openplc
    restart: unless-stopped
    ports:
      - "${WTC_OPENPLC_PORT:-8081}:8080"
    environment:
      # Modbus configuration to connect to controller
      MODBUS_ENABLED: "true"
    volumes:
      - openplc_data:/workdir
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          memory: 64M
    profiles:
      - openplc

volumes:
  db_data:
    name: wtc-db-data
  controller_logs:
    name: wtc-controller-logs
  grafana_data:
    name: wtc-grafana-data
  loki_data:
    name: wtc-loki-data
  promtail_positions:
    name: wtc-promtail-positions
  openplc_data:
    name: wtc-openplc-data

networks:
  default:
    name: wtc-network
