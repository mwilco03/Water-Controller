# Water Treatment Controller - Docker Compose
# Copyright (C) 2024
# SPDX-License-Identifier: GPL-3.0-or-later
#
# Port Configuration:
# All ports are configurable via environment variables.
# See config/ports.env for the single source of truth.
#
# Database Credentials (DEV/TEST ONLY):
# INTENTIONALLY HARDCODED - see CLAUDE.md
# - DB_USER: wtc
# - DB_PASSWORD: wtc_password (DO NOT change or externalize)
# - DB_NAME: water_treatment
# These appear in: database.environment.POSTGRES_PASSWORD, api.environment.DATABASE_URL
#
# Usage:
#   docker compose --env-file ../config/ports.env up -d
#
# Or set variables directly:
#   WTC_API_PORT=8000 WTC_UI_PORT=8080 docker compose up -d

services:
  # PostgreSQL with TimescaleDB
  database:
    image: timescale/timescaledb:latest-pg15
    container_name: wtc-database
    restart: unless-stopped
    environment:
      POSTGRES_USER: wtc
      POSTGRES_PASSWORD: wtc_password
      POSTGRES_DB: water_treatment
    volumes:
      - db_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    # Expose to localhost for API container (which uses host network)
    # Bind to 127.0.0.1 only for security (not exposed externally)
    ports:
      - "127.0.0.1:${WTC_DB_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U wtc -d water_treatment"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          memory: 256M

  # Web API
  # Uses host network mode to enable:
  # - PROFINET DCP discovery (raw Ethernet frames on physical interface)
  # - Network ping scans to reach RTUs on physical network
  # - Direct access to controller shared memory via host IPC namespace
  #
  # ICMP ping uses unprivileged mode (SOCK_DGRAM) by default via icmplib.
  # CAP_NET_RAW is only strictly required for DCP discovery (raw Ethernet).
  # If host has net.ipv4.ping_group_range set, ping works without CAP_NET_RAW.
  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile.web
    container_name: wtc-api
    restart: unless-stopped
    # Host network mode required for physical network access (DCP, ping)
    network_mode: host
    # Host IPC namespace required for POSIX shared memory with controller
    ipc: host
    # Capabilities for DCP discovery (raw Ethernet Layer 2 frames)
    # Based on Siemens Edgeshark and CISA Malcolm configurations:
    # - NET_RAW: raw socket creation (AF_PACKET)
    # - NET_ADMIN: network interface configuration (promiscuous mode)
    # Ref: https://github.com/siemens/edgeshark
    # Ref: https://github.com/cisagov/Malcolm
    cap_add:
      - NET_RAW
      - NET_ADMIN
    # Disable seccomp to allow AF_PACKET raw sockets for DCP discovery
    # Siemens Edgeshark uses same approach for industrial packet capture
    security_opt:
      - seccomp:unconfined
    environment:
      # Database accessed via localhost since we're on host network
      DATABASE_URL: postgresql://wtc:wtc_password@127.0.0.1:${WTC_DB_PORT:-5432}/water_treatment
      API_HOST: 0.0.0.0
      WTC_API_PORT: ${WTC_API_PORT:-8000}
      WTC_UI_PORT: ${WTC_UI_PORT:-8080}
      # Production mode - strict validation (industrial SCADA default)
      WTC_STARTUP_MODE: ${WTC_STARTUP_MODE:-production}
      # Real operations - NOT simulation
      WTC_SIMULATION_MODE: "false"
      WTC_DEMO_MODE: "false"
      # Skip UI asset validation (UI is served by separate container)
      WTC_API_ONLY: ${WTC_API_ONLY:-true}
      # Network interface for DCP discovery (same as controller)
      # Leave unset for auto-detection, or set WTC_INTERFACE in environment
      WTC_INTERFACE: ${WTC_INTERFACE:-}
      # Python PROFINET controller (runs in-process, no C controller needed)
      # RTUs are discovered via DCP multicast - no hardcoded IPs or names
      WTC_USE_PYTHON_CONTROLLER: ${WTC_USE_PYTHON_CONTROLLER:-1}
      # Shared memory name for POSIX IPC with controller (fallback if Python controller disabled)
      WTC_SHM_NAME: /wtc_shared_memory
    depends_on:
      database:
        condition: service_healthy
      controller:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8000/health').raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          memory: 128M

  # PROFINET Controller - REQUIRED for PROFINET communication
  # Provides: DCP discovery, AR management, cyclic I/O exchange
  # Communicates with API via POSIX shared memory in host IPC namespace
  controller:
    build:
      context: ..
      dockerfile: docker/Dockerfile.controller
      args:
        # Git version info for build identification (passed at build time)
        GIT_COMMIT: ${GIT_COMMIT:-unknown}
        GIT_DATE: ${GIT_DATE:-unknown}
    container_name: wtc-controller
    restart: unless-stopped
    network_mode: host
    # Host IPC namespace required for POSIX shared memory with API
    ipc: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    environment:
      # Network interface - leave unset for auto-detection, or set WTC_INTERFACE
      WTC_INTERFACE: ${WTC_INTERFACE:-}
      WTC_CYCLE_TIME: ${WTC_CYCLE_TIME:-1000}
      WTC_LOG_LEVEL: ${WTC_LOG_LEVEL:-INFO}
      # Shared memory name for POSIX IPC (created in /dev/shm/wtc_shared_memory)
      WTC_SHM_NAME: /wtc_shared_memory
    volumes:
      - ./config:/etc/water-controller:ro
      - controller_logs:/var/log/water-controller
    healthcheck:
      # Check BOTH that process is running AND shared memory exists
      # This prevents API from starting before controller has initialized IPC
      test: ["CMD-SHELL", "pgrep -f water_treat_controller && test -e /dev/shm/wtc_shared_memory"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 512M
        reservations:
          memory: 128M
    # NO profiles - controller starts by default
    # This is required for PROFINET operations

  # React HMI Frontend
  ui:
    build:
      context: ..
      dockerfile: docker/Dockerfile.ui
      args:
        # API URL - use host.docker.internal to reach API on host network
        API_URL: http://host.docker.internal:${WTC_API_PORT:-8000}
    container_name: wtc-ui
    restart: unless-stopped
    user: "1000:1000"
    environment:
      # API URL for server-side requests (host network)
      API_URL: http://host.docker.internal:${WTC_API_PORT:-8000}
      # Public API URL for client-side (empty = use same origin with rewrites)
      NEXT_PUBLIC_API_URL: ""
      WTC_UI_PORT: ${WTC_UI_PORT:-8080}
      WTC_API_PORT: ${WTC_API_PORT:-8000}
    ports:
      - "${WTC_UI_PORT:-8080}:${WTC_DOCKER_UI_INTERNAL_PORT:-3000}"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:${WTC_DOCKER_UI_INTERNAL_PORT:-3000}"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          memory: 64M

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: wtc-grafana
    restart: unless-stopped
    # DCK-H1 fix: Run as grafana user
    user: "472:472"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      # Grafana Credentials (DEV/TEST ONLY):
      # INTENTIONALLY HARDCODED - see CLAUDE.md
      # - GF_SECURITY_ADMIN_USER: admin
      # - GF_SECURITY_ADMIN_PASSWORD: admin (DO NOT change or externalize)
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource
      GF_SERVER_HTTP_PORT: ${WTC_GRAFANA_PORT:-3000}
      # Unified alerting (disabled by default - enable when ready)
      GF_UNIFIED_ALERTING_ENABLED: ${GF_UNIFIED_ALERTING_ENABLED:-true}
      GF_ALERTING_ENABLED: "false"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    ports:
      - "${WTC_GRAFANA_PORT:-3000}:${WTC_GRAFANA_PORT:-3000}"
    depends_on:
      loki:
        condition: service_healthy
      database:
        condition: service_healthy
    # DCK-H2 fix: Add health check
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:${WTC_GRAFANA_PORT:-3000}/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    # DCK-H3 fix: Add resource limits
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          memory: 64M

  # Loki - Centralized log aggregation
  loki:
    image: grafana/loki:2.9.4
    container_name: wtc-loki
    restart: unless-stopped
    command: -config.file=/etc/loki/loki-config.yml
    volumes:
      - ./loki/loki-config.yml:/etc/loki/loki-config.yml:ro
      - loki_data:/loki
    expose:
      - "3100"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3100/ready"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          memory: 128M

  # Promtail - Log collector (ships Docker logs to Loki)
  promtail:
    image: grafana/promtail:2.9.4
    container_name: wtc-promtail
    restart: unless-stopped
    command: -config.file=/etc/promtail/promtail-config.yml
    volumes:
      - ./promtail/promtail-config.yml:/etc/promtail/promtail-config.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - promtail_positions:/var/promtail
    depends_on:
      loki:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9080/ready"]
      interval: 15s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          memory: 32M

  # OpenPLC Editor (Read-Only Ladder Logic Viewer)
  # Provides browser-based ladder logic visualization with Modbus communication
  openplc:
    image: openplcproject/openplc_v3:latest
    container_name: wtc-openplc
    restart: unless-stopped
    ports:
      - "${WTC_OPENPLC_PORT:-8081}:8080"
    environment:
      # Modbus configuration to connect to controller
      MODBUS_ENABLED: "true"
    volumes:
      - openplc_data:/workdir
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          memory: 64M
    profiles:
      - openplc

volumes:
  db_data:
    name: wtc-db-data
  controller_logs:
    name: wtc-controller-logs
  grafana_data:
    name: wtc-grafana-data
  loki_data:
    name: wtc-loki-data
  promtail_positions:
    name: wtc-promtail-positions
  openplc_data:
    name: wtc-openplc-data

networks:
  default:
    name: wtc-network
